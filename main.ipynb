{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from SnakeGameEnv import SnakeEnv\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "from SmartSnake import BaseSnake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "\n",
    "        self.layer_sizes = np.array([self.state_dim,self.num_hidden_units,self.num_actions])\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
    "        \n",
    "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
    "\n",
    "        for i in range(0,len(self.layer_sizes) - 1):\n",
    "            ins, out = self.layer_sizes[i], self.layer_sizes[i + 1]\n",
    "            self.weights[i]['W'] = self.init_saxe(ins, out)\n",
    "            self.weights[i]['b'] = np.zeros((1,out))\n",
    "    def get_action_values(self,s):\n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "\n",
    "        psi = np.dot(s,W0) + b0\n",
    "        x = np.max(psi,0)\n",
    "\n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        q_values = np.dot(x,W1) + b1\n",
    "\n",
    "        return q_values\n",
    "    def td_update(self, s, delta_mat):\n",
    "        W0,b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        W1,b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "\n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        dx = (psi > 0).astype(float)\n",
    "\n",
    "        td_update = [dict() for i in nrange(len(self.weights))]\n",
    "\n",
    "        v = delta_mat\n",
    "        td_update[1]['W'] = np.dot(x.T , v) * 1. / s.shape[0]\n",
    "        td_update[1]['b'] = np.sum(v, axis = 0, keepdims=True) * 1. / s.shape[0]\n",
    "\n",
    "        v = np.dot(v, W1.T) * dx\n",
    "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "        td_update[0]['b'] = np.sum(v, axis = 0, keepdims=True) * 1./ s.shape[0]\n",
    "\n",
    "        return td_update\n",
    "\n",
    "    #The nonlinear dy\n",
    "    def init_saxe(self,rows,cols):\n",
    "        tensor = self.rand_generator.normal(0,1, (rows,cols))\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        #The np.linalg.qr solve the least square problem\n",
    "        tensor, r = np.linalg.qr(tensor)\n",
    "        \n",
    "        d = np.diag(r, 0)\n",
    "\n",
    "        ph = np.sign(d)\n",
    "\n",
    "        tensor *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        return tensor\n",
    "    def get_weights(self):\n",
    "        return deepcopy(self.weights)\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = deepcopy(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5 20  3]\n"
     ]
    }
   ],
   "source": [
    "#Checking the result of layer sizes\n",
    "network_config = {\n",
    "    \"state_dim\": 5,\n",
    "    \"num_hidden_units\":20,\n",
    "    \"num_actions\": 3\n",
    "}\n",
    "test_network = ActionValueNetwork(network_config)\n",
    "print(test_network.layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam Optimizer \n",
    "class Adam():\n",
    "    def __init__(self, layer_sizes, optimizer_info):\n",
    "        '''\n",
    "            optimizer_info = {\n",
    "                step_size = None\n",
    "                beta_m = None\n",
    "                beta_v = None\n",
    "                epsilon = None\n",
    "            }\n",
    "\n",
    "        '''\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "\n",
    "        self.m = [dict() for i in range(1 , len(self.layer_sizes))]\n",
    "        self.v = [dict() for i in range(1 , len(self.layer_sizes))]\n",
    "\n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            ins, out = self.layer_sizes[i], self.layer_sizes[i + 1]\n",
    "            self.m[i]['W'] = np.zeros((ins, out))\n",
    "            self.m[i]['b'] = np.zeros((1,out))\n",
    "            self.v[i]['W'] = np.zeros((ins, out))\n",
    "            self.v[i]['b'] = np.zeros((1,out))\n",
    "\n",
    "        self.beta_update_m = self.m\n",
    "        self.beta_update_v = self.v\n",
    "\n",
    "    def adam_optimize(self,weights, grads):\n",
    "        for i in range(0,len(self.layer_sizes) - 1):\n",
    "            for param in self.weights[i].keys():\n",
    "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * grads[i][param]\n",
    "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * grads[i][param]**2\n",
    "\n",
    "                m_hat = self.m[i][param] / (1 - self.beta_update_m)\n",
    "                v_hat = self.m[i][param] / (1 - self.beta_update_v)\n",
    "\n",
    "                weights[i][param]+= self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "        self.beta_update_m*=self.beta_m\n",
    "        self.beta_update_v*=self.beta_v\n",
    "\n",
    "        return weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Policy\n",
    "def softmax(action_val,tau = 1.0):\n",
    "    preferences = action_val / tau\n",
    "\n",
    "    max_pref = np.max(preferences)\n",
    "\n",
    "    max_pref = np.reshape(max_pref,(-1,1))\n",
    "\n",
    "    numerator = np.exp(preferences - max_pref)\n",
    "\n",
    "    denominator = np.sum(np.exp(preferences - max_pref),axis = 1)\n",
    "\n",
    "    denominator = np.reshape(denominator,(-1,1))\n",
    "\n",
    "    probs = numerator / denominator\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.28379663 3.70149596 2.49853409]\n",
      " [3.25031134 3.26791286 1.95532675]]\n",
      "[[0.05108911 0.87041499 0.0784959 ]\n",
      " [0.47374228 0.49071646 0.03554126]]\n"
     ]
    }
   ],
   "source": [
    "#Checking Softmax Policy\n",
    "\n",
    "tau = 0.5\n",
    "action_val = np.random.normal(3,1,size = (2,3))\n",
    "print(action_val)\n",
    "sm = softmax(action_val,tau)\n",
    "\n",
    "print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_TD_error(states,next_states, actions, reward, discount, terminals, network, current_q, tau):\n",
    "    next_mat = current_q.get_state_values(next_states)\n",
    "\n",
    "    policy = softmax(next_mat, tau)\n",
    "\n",
    "    v_next_vec = np.sum(next_mat * policy,axis = 1) * (1 - terminals)\n",
    "\n",
    "    target = reward - discount * v_next_vec\n",
    "\n",
    "    cur_mat = network.get_state_values(states)\n",
    "\n",
    "    batch_indices = np.arange(cur_mat.shape[0])\n",
    "\n",
    "    q_vec = cur_mat[batch_indices,actions]\n",
    "\n",
    "    delta_vec = target - q_vec\n",
    "\n",
    "    return delta_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        self.buffer = []\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.max_size = size\n",
    "    def append(self,state,action,reward,terminal, next_state):\n",
    "\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state,action,reward,terminal,next_state])\n",
    "\n",
    "    def sample(self):\n",
    "\n",
    "        index = self.rand_generator.choice(np.arange(len(self.buffer)), size = self.minibatch_size)\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "    def size(seed):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences,discount,optimizer,network,current_q,tau):\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    \n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "\n",
    "    delta_vec = Get_TD_error(states,next_states,actions,rewards,discount,terminals,network,current_q,tau)\n",
    "\n",
    "    batch_indices = np.arange(batch_size)\n",
    "\n",
    "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
    "    delta_mat[batch_indices, actions] = delta_vec\n",
    "\n",
    "    td_update = network.get_TD_update(states,delta_mat)\n",
    "\n",
    "    weights = optimizer.update_weights(network.get_weights(),td_update)\n",
    "\n",
    "    network.set_weights(weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseSnake):\n",
    "    def __init__(self):\n",
    "        self.name = \"Expected Sarsa Agent\"\n",
    "    def agent_init(self, agent_config):\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['buffer_size'],agent_config['buffer_minibatch_size'],agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimize = Adam(self.layer_sizes, agent_config['optimizer_config'])\n",
    "        self.tau = agent_config['tau']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.num_replay = agent_config['num_replay']\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "\n",
    "        self.rand_generator = np.random.RandomState(agent_config.get('seed'))\n",
    "\n",
    "        self.sum_reward = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "        self.last_action = None\n",
    "        self.last_state = None\n",
    "\n",
    "    def policy(self,state):\n",
    "        action_value = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_value,self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_start(self,state):\n",
    "        self.sum_reward = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        \n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self,reward,state):\n",
    "\n",
    "        self.sum_reward+=reward\n",
    "        self.episode_steps+=1\n",
    "\n",
    "        state = np.array([state])\n",
    "\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        self.replay_buffer.append(self.last_state,self.last_action,reward,0,state)\n",
    "\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                experiences = self.replay_buffer.sample()\n",
    "\n",
    "                optimize_network(experiences,self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "\n",
    "\n",
    "        self.last_action = action\n",
    "        self.last_state = state\n",
    "\n",
    "    def agent_end(self,reward):\n",
    "        self.sum_reward+=reward\n",
    "        self.episode_steps+=1\n",
    "\n",
    "        state = np.array([self.last_state])\n",
    "        action = self.policy(state)\n",
    "\n",
    "        self.replay_buffer.append(self.last_state,self.last_action,reward,1,state)\n",
    "\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "\n",
    "                experiences = self.replay_buffer.sample()\n",
    "\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "\n",
    "    def agent_message(self,message):\n",
    "        if message == 'get_sum_reward':\n",
    "            return self.sum_reward\n",
    "        else: raise Exception('Unrecognize Message')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "\n",
    "    agent_sum_reward = np.zeros((experiment_parameters['num_runs'],experiment_parameters['num_episodes']))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    for run in range(1, experiment_parameters['num_runs'] + 1):\n",
    "        agent_info['seed'] = run\n",
    "        agent_info['network_config']['seed'] = run\n",
    "        env_info['seed'] = run\n",
    "        \n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "        for episode in tqdm(range(1, experiment_parameters['replay'])):\n",
    "\n",
    "            rl_glue.rl_episode(experiment_parameters['timeout'])\n",
    "\n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save('results/sum_reward_{}'.format(save_name),agent_info)\n",
    "    shutil.make_archive('results','zip','results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "env_init() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-6faf3ecb48ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mcurrent_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menvironment_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-ef56701e3535>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(environment, agent, environment_parameters, agent_parameters, experiment_parameters)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0menv_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mrl_glue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'replay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SmartSnake/rl_glue.py\u001b[0m in \u001b[0;36mrl_init\u001b[0;34m(self, agent_init_info, env_init_info)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrl_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_init_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_init_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m\"\"\"Initial method called when RLGlue experiment is created\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_init_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_init_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: env_init() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "experiment_parameters = {\n",
    "    'num_runs': 1,\n",
    "    'num_episodes':300,\n",
    "    'timeout':1000\n",
    "}\n",
    "\n",
    "environment_parameters = {\n",
    "    \n",
    "}\n",
    "\n",
    "current_env = SnakeEnv\n",
    "\n",
    "agent_parameters = {\n",
    "    'network_config':{\n",
    "        'state_dim':8,\n",
    "        'num_hidden_units':256,\n",
    "        'num_actions':4,\n",
    "    },\n",
    "    'optimizer_config':{\n",
    "        'step_size':1e-3,\n",
    "        'beta_m':0.9,\n",
    "        'beta_v':0.999,\n",
    "        'epsilon': 1e-8,\n",
    "    },\n",
    "    'buffer_size': 50000,\n",
    "    'num_replay': 8,\n",
    "    'minibatch_size': 4,\n",
    "    'gamma': 0.99,\n",
    "    'epsilon': 0.001\n",
    "}\n",
    "\n",
    "current_agent = Agent\n",
    "\n",
    "run_experiment(current_env,current_agent,environment_parameters, agent_parameters,experiment_parameters)\n",
    "\n",
    "training_NN = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
